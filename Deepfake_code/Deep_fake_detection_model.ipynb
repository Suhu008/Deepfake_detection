{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.fftpack import dct      # importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Custom Dataset: includes RGB, LBP, and DCT\n",
    "# ------------------------------------------\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, transform=None):\n",
    "        self.dataset = image_folder_dataset\n",
    "        self.transform = transform\n",
    "        self.resize = transforms.Resize((128, 128))  # Ensure consistent input size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        \n",
    "        # Resize image to 128x128 before further processing\n",
    "        resized_image = self.resize(image)\n",
    "        image_np = np.array(resized_image)\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        gray_image = np.array(Image.fromarray(image_np).convert('L')) if image_np.ndim == 3 else image_np\n",
    "\n",
    "        # ----------- LBP Feature Extraction -----------\n",
    "        lbp = local_binary_pattern(gray_image, 8, 1, method='uniform')\n",
    "        lbp = ((lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-8) * 255).astype(np.uint8)\n",
    "\n",
    "        # ----------- DCT Feature Extraction -----------\n",
    "        h, w = gray_image.shape\n",
    "        dct_block_size = 8\n",
    "        dct_feature = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "        for i in range(0, h, dct_block_size):\n",
    "            for j in range(0, w, dct_block_size):\n",
    "                if i + dct_block_size <= h and j + dct_block_size <= w:\n",
    "                    block = gray_image[i:i+dct_block_size, j:j+dct_block_size]\n",
    "                    dct_block = dct(dct(block.T, norm='ortho').T, norm='ortho')\n",
    "                    dct_feature[i:i+dct_block_size, j:j+dct_block_size] = dct_block\n",
    "\n",
    "        dct_feature = ((dct_feature - dct_feature.min()) / (dct_feature.max() - dct_feature.min() + 1e-8) * 255).astype(np.uint8)\n",
    "\n",
    "        # ----------- Transform image tensor (RGB) -----------\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(resized_image)\n",
    "\n",
    "        # Convert LBP and DCT features to PyTorch tensors\n",
    "        lbp_tensor = torch.from_numpy(lbp).float().unsqueeze(0) / 255.0\n",
    "        dct_tensor = torch.from_numpy(dct_feature).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        # Normalize LBP & DCT features\n",
    "        lbp_tensor = (lbp_tensor - 0.5) / 0.225\n",
    "        dct_tensor = (dct_tensor - 0.5) / 0.225\n",
    "\n",
    "        # Concatenate RGB, LBP, and DCT (total 5 channels)\n",
    "        combined_tensor = torch.cat([image_tensor, lbp_tensor, dct_tensor], dim=0)\n",
    "        \n",
    "        return combined_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Self-Attention Layer for each branch\n",
    "# ------------------------------------------\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # Learnable scaling\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.size()\n",
    "        query = self.query(x).view(batch_size, -1, H*W).permute(0, 2, 1)  # (B, N, C')   B= Batch Size  C = No of Channels   C' = Reduced size of channel   H,W = height,width \n",
    "        key = self.key(x).view(batch_size, -1, H*W)                       # (B, C', N)\n",
    "        value = self.value(x).view(batch_size, -1, H*W)                   # (B, C, N)\n",
    "        \n",
    "        attention = self.softmax(torch.bmm(query, key))                  # (B, N, N)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))               # (B, C, N)\n",
    "        out = out.view(batch_size, C, H, W)                              # (B, C, H, W)\n",
    "        \n",
    "        return self.gamma * out + x  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310699f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Full Deepfake Detection Model with 3 branches\n",
    "# ------------------------------------------\n",
    "class DeepfakeDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepfakeDetectionModel, self).__init__()\n",
    "\n",
    "        # Image CNN branch\n",
    "        self.image_branch = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.image_attention = SelfAttention(128)\n",
    "\n",
    "        # LBP branch\n",
    "        self.lbp_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.lbp_attention = SelfAttention(128)\n",
    "\n",
    "        # DCT branch\n",
    "        self.dct_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dct_attention = SelfAttention(128)\n",
    "\n",
    "        # Flatten layers\n",
    "        self.image_flatten = nn.Flatten()\n",
    "        self.lbp_flatten = nn.Flatten()\n",
    "        self.dct_flatten = nn.Flatten()\n",
    "\n",
    "        # Feature dimensions\n",
    "        self.image_feature_dim = 128 * 16 * 16\n",
    "        self.lbp_feature_dim = 128 * 16 * 16\n",
    "        self.dct_feature_dim = 128 * 16 * 16\n",
    "\n",
    "        # Fusion & Classification\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim + self.lbp_feature_dim + self.dct_feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_loss = []\n",
    "        self.val_accuracy = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into RGB, LBP, DCT\n",
    "        image = x[:, :3, :, :]\n",
    "        lbp = x[:, 3:4, :, :]\n",
    "        dct = x[:, 4:5, :, :]\n",
    "\n",
    "        # Apply CNN + attention for each stream\n",
    "        image_features = self.image_branch(image)\n",
    "        lbp_features = self.lbp_branch(lbp)\n",
    "        dct_features = self.dct_branch(dct)\n",
    "\n",
    "        image_features = self.image_attention(image_features)\n",
    "        lbp_features = self.lbp_attention(lbp_features)\n",
    "        dct_features = self.dct_attention(dct_features)\n",
    "\n",
    "        image_features = self.image_flatten(image_features)\n",
    "        lbp_features = self.lbp_flatten(lbp_features)\n",
    "        dct_features = self.dct_flatten(dct_features)\n",
    "        # Flatten and concatenate\n",
    "        combined = torch.cat((image_features, lbp_features, dct_features), dim=1)\n",
    "        output = self.fusion(combined)\n",
    "        return output\n",
    "    # ------------------------------------------\n",
    "    # Model Training Loop\n",
    "    # ------------------------------------------\n",
    "    def train_model(self, train_loader, valid_loader, num_epochs, device):\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True, min_lr=1e-6)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Training...\")\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * images.size(0)\n",
    "                predicted_labels = (outputs >= 0.0).float()\n",
    "                correct_train += (predicted_labels == labels.unsqueeze(1)).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"Batch [{batch_idx+1}/{len(train_loader)}] - \"\n",
    "                          f\"Loss: {loss.item():.4f} - Train Accuracy: {correct_train / total_train:.4f}\")\n",
    "\n",
    "            average_loss = total_loss / len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "\n",
    "            self.train_loss.append(average_loss)\n",
    "            self.train_accuracy.append(train_accuracy)\n",
    "\n",
    "            val_loss, val_accuracy = self.evaluate_model(valid_loader, device, eval_mode='Validation')\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_accuracy.append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {average_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - \"\n",
    "                  f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f} - \"\n",
    "                  f\"LR: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "    # ------------------------------------------\n",
    "    # Evaluation for Validation and Test Set\n",
    "    # ------------------------------------------\n",
    "    def evaluate_model(self, data_loader, device, eval_mode='Test'):\n",
    "        self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "                total_loss += loss.item() * images.size(0)\n",
    "\n",
    "                predicted_labels = (outputs >= 0.0).float()\n",
    "                correct += (predicted_labels == labels.unsqueeze(1)).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "        average_loss = total_loss / len(data_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(f\"\\n{eval_mode} Loss: {average_loss:.4f} - {eval_mode} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if eval_mode == 'Test':\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred))\n",
    "\n",
    "        return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c988fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20] - Training...\n",
      "Batch [10/391] - Loss: 2.8962 - Train Accuracy: 0.4969\n",
      "Batch [20/391] - Loss: 1.1342 - Train Accuracy: 0.5062\n",
      "Batch [30/391] - Loss: 0.8650 - Train Accuracy: 0.5073\n",
      "Batch [40/391] - Loss: 0.7600 - Train Accuracy: 0.5027\n",
      "Batch [50/391] - Loss: 0.7060 - Train Accuracy: 0.5028\n",
      "Batch [60/391] - Loss: 0.6785 - Train Accuracy: 0.5086\n",
      "Batch [70/391] - Loss: 0.6326 - Train Accuracy: 0.5196\n",
      "Batch [80/391] - Loss: 0.6984 - Train Accuracy: 0.5240\n",
      "Batch [90/391] - Loss: 0.6799 - Train Accuracy: 0.5297\n",
      "Batch [100/391] - Loss: 0.6627 - Train Accuracy: 0.5364\n",
      "Batch [110/391] - Loss: 0.5917 - Train Accuracy: 0.5435\n",
      "Batch [120/391] - Loss: 0.6634 - Train Accuracy: 0.5520\n",
      "Batch [130/391] - Loss: 0.6216 - Train Accuracy: 0.5573\n",
      "Batch [140/391] - Loss: 0.6616 - Train Accuracy: 0.5590\n",
      "Batch [150/391] - Loss: 0.5683 - Train Accuracy: 0.5640\n",
      "Batch [160/391] - Loss: 0.7065 - Train Accuracy: 0.5676\n",
      "Batch [170/391] - Loss: 0.6313 - Train Accuracy: 0.5712\n",
      "Batch [180/391] - Loss: 0.6249 - Train Accuracy: 0.5748\n",
      "Batch [190/391] - Loss: 0.6143 - Train Accuracy: 0.5781\n",
      "Batch [200/391] - Loss: 0.5940 - Train Accuracy: 0.5808\n",
      "Batch [210/391] - Loss: 0.7318 - Train Accuracy: 0.5841\n",
      "Batch [220/391] - Loss: 0.6320 - Train Accuracy: 0.5879\n",
      "Batch [230/391] - Loss: 0.6309 - Train Accuracy: 0.5915\n",
      "Batch [240/391] - Loss: 0.5426 - Train Accuracy: 0.5947\n",
      "Batch [250/391] - Loss: 0.6124 - Train Accuracy: 0.5974\n",
      "Batch [260/391] - Loss: 0.6353 - Train Accuracy: 0.6010\n",
      "Batch [270/391] - Loss: 0.5012 - Train Accuracy: 0.6045\n",
      "Batch [280/391] - Loss: 0.5749 - Train Accuracy: 0.6069\n",
      "Batch [290/391] - Loss: 0.5883 - Train Accuracy: 0.6086\n",
      "Batch [300/391] - Loss: 0.6192 - Train Accuracy: 0.6105\n",
      "Batch [310/391] - Loss: 0.6055 - Train Accuracy: 0.6128\n",
      "Batch [320/391] - Loss: 0.6169 - Train Accuracy: 0.6151\n",
      "Batch [330/391] - Loss: 0.5595 - Train Accuracy: 0.6176\n",
      "Batch [340/391] - Loss: 0.5689 - Train Accuracy: 0.6202\n",
      "Batch [350/391] - Loss: 0.6516 - Train Accuracy: 0.6219\n",
      "Batch [360/391] - Loss: 0.5935 - Train Accuracy: 0.6241\n",
      "Batch [370/391] - Loss: 0.5771 - Train Accuracy: 0.6259\n",
      "Batch [380/391] - Loss: 0.5185 - Train Accuracy: 0.6275\n",
      "Batch [390/391] - Loss: 0.5633 - Train Accuracy: 0.6306\n",
      "\n",
      "Validation Loss: 0.5388 - Validation Accuracy: 0.7524\n",
      "Epoch [1/20] - Train Loss: 0.8311 - Train Accuracy: 0.6306 - Val Loss: 0.5388 - Val Accuracy: 0.7524 - LR: 0.001000\n",
      "\n",
      "Epoch [2/20] - Training...\n",
      "Batch [10/391] - Loss: 0.6235 - Train Accuracy: 0.7328\n",
      "Batch [20/391] - Loss: 0.5588 - Train Accuracy: 0.7305\n",
      "Batch [30/391] - Loss: 0.5529 - Train Accuracy: 0.7250\n",
      "Batch [40/391] - Loss: 0.5987 - Train Accuracy: 0.7215\n",
      "Batch [50/391] - Loss: 0.6019 - Train Accuracy: 0.7222\n",
      "Batch [60/391] - Loss: 0.5313 - Train Accuracy: 0.7229\n",
      "Batch [70/391] - Loss: 0.4393 - Train Accuracy: 0.7232\n",
      "Batch [80/391] - Loss: 0.6465 - Train Accuracy: 0.7227\n",
      "Batch [90/391] - Loss: 0.5931 - Train Accuracy: 0.7259\n",
      "Batch [100/391] - Loss: 0.5918 - Train Accuracy: 0.7262\n",
      "Batch [110/391] - Loss: 0.5754 - Train Accuracy: 0.7260\n",
      "Batch [120/391] - Loss: 0.7442 - Train Accuracy: 0.7255\n",
      "Batch [130/391] - Loss: 0.6299 - Train Accuracy: 0.7239\n",
      "Batch [140/391] - Loss: 0.4102 - Train Accuracy: 0.7276\n",
      "Batch [150/391] - Loss: 0.6025 - Train Accuracy: 0.7299\n",
      "Batch [160/391] - Loss: 0.5211 - Train Accuracy: 0.7308\n",
      "Batch [170/391] - Loss: 0.5354 - Train Accuracy: 0.7310\n",
      "Batch [180/391] - Loss: 0.5620 - Train Accuracy: 0.7335\n",
      "Batch [190/391] - Loss: 0.3783 - Train Accuracy: 0.7355\n",
      "Batch [200/391] - Loss: 0.4082 - Train Accuracy: 0.7366\n",
      "Batch [210/391] - Loss: 0.4017 - Train Accuracy: 0.7372\n",
      "Batch [220/391] - Loss: 0.4639 - Train Accuracy: 0.7395\n",
      "Batch [230/391] - Loss: 0.4157 - Train Accuracy: 0.7408\n",
      "Batch [240/391] - Loss: 0.5815 - Train Accuracy: 0.7411\n",
      "Batch [250/391] - Loss: 0.6175 - Train Accuracy: 0.7417\n",
      "Batch [260/391] - Loss: 0.4965 - Train Accuracy: 0.7424\n",
      "Batch [270/391] - Loss: 0.4254 - Train Accuracy: 0.7417\n",
      "Batch [280/391] - Loss: 0.5153 - Train Accuracy: 0.7432\n",
      "Batch [290/391] - Loss: 0.4368 - Train Accuracy: 0.7442\n",
      "Batch [300/391] - Loss: 0.5244 - Train Accuracy: 0.7447\n",
      "Batch [310/391] - Loss: 0.5140 - Train Accuracy: 0.7460\n",
      "Batch [320/391] - Loss: 0.3888 - Train Accuracy: 0.7468\n",
      "Batch [330/391] - Loss: 0.4340 - Train Accuracy: 0.7484\n",
      "Batch [340/391] - Loss: 0.3260 - Train Accuracy: 0.7499\n",
      "Batch [350/391] - Loss: 0.4268 - Train Accuracy: 0.7516\n",
      "Batch [360/391] - Loss: 0.4462 - Train Accuracy: 0.7533\n",
      "Batch [370/391] - Loss: 0.4952 - Train Accuracy: 0.7543\n",
      "Batch [380/391] - Loss: 0.3948 - Train Accuracy: 0.7554\n",
      "Batch [390/391] - Loss: 0.3938 - Train Accuracy: 0.7565\n",
      "\n",
      "Validation Loss: 0.5141 - Validation Accuracy: 0.7356\n",
      "Epoch [2/20] - Train Loss: 0.5089 - Train Accuracy: 0.7566 - Val Loss: 0.5141 - Val Accuracy: 0.7356 - LR: 0.001000\n",
      "\n",
      "Epoch [3/20] - Training...\n",
      "Batch [10/391] - Loss: 0.5185 - Train Accuracy: 0.8000\n",
      "Batch [20/391] - Loss: 0.3814 - Train Accuracy: 0.8133\n",
      "Batch [30/391] - Loss: 0.4885 - Train Accuracy: 0.8177\n",
      "Batch [40/391] - Loss: 0.4622 - Train Accuracy: 0.8172\n",
      "Batch [50/391] - Loss: 0.4990 - Train Accuracy: 0.8147\n",
      "Batch [60/391] - Loss: 0.4111 - Train Accuracy: 0.8159\n",
      "Batch [70/391] - Loss: 0.3994 - Train Accuracy: 0.8179\n",
      "Batch [80/391] - Loss: 0.4442 - Train Accuracy: 0.8166\n",
      "Batch [90/391] - Loss: 0.3654 - Train Accuracy: 0.8135\n",
      "Batch [100/391] - Loss: 0.3748 - Train Accuracy: 0.8139\n",
      "Batch [110/391] - Loss: 0.3820 - Train Accuracy: 0.8135\n",
      "Batch [120/391] - Loss: 0.2932 - Train Accuracy: 0.8132\n",
      "Batch [130/391] - Loss: 0.3259 - Train Accuracy: 0.8141\n",
      "Batch [140/391] - Loss: 0.3484 - Train Accuracy: 0.8153\n",
      "Batch [150/391] - Loss: 0.3181 - Train Accuracy: 0.8178\n",
      "Batch [160/391] - Loss: 0.4310 - Train Accuracy: 0.8176\n",
      "Batch [170/391] - Loss: 0.3494 - Train Accuracy: 0.8189\n",
      "Batch [180/391] - Loss: 0.4799 - Train Accuracy: 0.8189\n",
      "Batch [190/391] - Loss: 0.3263 - Train Accuracy: 0.8181\n",
      "Batch [200/391] - Loss: 0.3915 - Train Accuracy: 0.8196\n",
      "Batch [210/391] - Loss: 0.4445 - Train Accuracy: 0.8220\n",
      "Batch [220/391] - Loss: 0.4222 - Train Accuracy: 0.8226\n",
      "Batch [230/391] - Loss: 0.3527 - Train Accuracy: 0.8225\n",
      "Batch [240/391] - Loss: 0.4375 - Train Accuracy: 0.8244\n",
      "Batch [250/391] - Loss: 0.4937 - Train Accuracy: 0.8254\n",
      "Batch [260/391] - Loss: 0.3281 - Train Accuracy: 0.8260\n",
      "Batch [270/391] - Loss: 0.3780 - Train Accuracy: 0.8254\n",
      "Batch [280/391] - Loss: 0.3242 - Train Accuracy: 0.8268\n",
      "Batch [290/391] - Loss: 0.3198 - Train Accuracy: 0.8270\n",
      "Batch [300/391] - Loss: 0.3293 - Train Accuracy: 0.8271\n",
      "Batch [310/391] - Loss: 0.3095 - Train Accuracy: 0.8280\n",
      "Batch [320/391] - Loss: 0.3644 - Train Accuracy: 0.8285\n",
      "Batch [330/391] - Loss: 0.1982 - Train Accuracy: 0.8299\n",
      "Batch [340/391] - Loss: 0.3436 - Train Accuracy: 0.8307\n",
      "Batch [350/391] - Loss: 0.6276 - Train Accuracy: 0.8311\n",
      "Batch [360/391] - Loss: 0.4602 - Train Accuracy: 0.8314\n",
      "Batch [370/391] - Loss: 0.3337 - Train Accuracy: 0.8309\n",
      "Batch [380/391] - Loss: 0.3614 - Train Accuracy: 0.8316\n",
      "Batch [390/391] - Loss: 0.4546 - Train Accuracy: 0.8315\n",
      "\n",
      "Validation Loss: 0.3562 - Validation Accuracy: 0.8632\n",
      "Epoch [3/20] - Train Loss: 0.3803 - Train Accuracy: 0.8316 - Val Loss: 0.3562 - Val Accuracy: 0.8632 - LR: 0.001000\n",
      "\n",
      "Epoch [4/20] - Training...\n",
      "Batch [10/391] - Loss: 0.2965 - Train Accuracy: 0.8859\n",
      "Batch [20/391] - Loss: 0.2750 - Train Accuracy: 0.8719\n",
      "Batch [30/391] - Loss: 0.4549 - Train Accuracy: 0.8760\n",
      "Batch [40/391] - Loss: 0.2513 - Train Accuracy: 0.8801\n",
      "Batch [50/391] - Loss: 0.2673 - Train Accuracy: 0.8778\n",
      "Batch [60/391] - Loss: 0.2623 - Train Accuracy: 0.8768\n",
      "Batch [70/391] - Loss: 0.3363 - Train Accuracy: 0.8732\n",
      "Batch [80/391] - Loss: 0.2978 - Train Accuracy: 0.8744\n",
      "Batch [90/391] - Loss: 0.2136 - Train Accuracy: 0.8740\n",
      "Batch [100/391] - Loss: 0.2423 - Train Accuracy: 0.8727\n",
      "Batch [110/391] - Loss: 0.1649 - Train Accuracy: 0.8754\n",
      "Batch [120/391] - Loss: 0.3257 - Train Accuracy: 0.8751\n",
      "Batch [130/391] - Loss: 0.3771 - Train Accuracy: 0.8758\n",
      "Batch [140/391] - Loss: 0.2569 - Train Accuracy: 0.8770\n",
      "Batch [150/391] - Loss: 0.4097 - Train Accuracy: 0.8774\n",
      "Batch [160/391] - Loss: 0.3529 - Train Accuracy: 0.8766\n",
      "Batch [170/391] - Loss: 0.3071 - Train Accuracy: 0.8780\n",
      "Batch [180/391] - Loss: 0.4228 - Train Accuracy: 0.8773\n",
      "Batch [190/391] - Loss: 0.3096 - Train Accuracy: 0.8766\n",
      "Batch [200/391] - Loss: 0.3962 - Train Accuracy: 0.8766\n",
      "Batch [210/391] - Loss: 0.3315 - Train Accuracy: 0.8768\n",
      "Batch [220/391] - Loss: 0.3071 - Train Accuracy: 0.8775\n",
      "Batch [230/391] - Loss: 0.2353 - Train Accuracy: 0.8779\n",
      "Batch [240/391] - Loss: 0.2325 - Train Accuracy: 0.8790\n",
      "Batch [250/391] - Loss: 0.2946 - Train Accuracy: 0.8791\n",
      "Batch [260/391] - Loss: 0.2445 - Train Accuracy: 0.8796\n",
      "Batch [270/391] - Loss: 0.1645 - Train Accuracy: 0.8801\n",
      "Batch [280/391] - Loss: 0.2474 - Train Accuracy: 0.8802\n",
      "Batch [290/391] - Loss: 0.3264 - Train Accuracy: 0.8807\n",
      "Batch [300/391] - Loss: 0.1790 - Train Accuracy: 0.8811\n",
      "Batch [310/391] - Loss: 0.4311 - Train Accuracy: 0.8818\n",
      "Batch [320/391] - Loss: 0.2295 - Train Accuracy: 0.8823\n",
      "Batch [330/391] - Loss: 0.2157 - Train Accuracy: 0.8822\n",
      "Batch [340/391] - Loss: 0.2422 - Train Accuracy: 0.8825\n",
      "Batch [350/391] - Loss: 0.4063 - Train Accuracy: 0.8824\n",
      "Batch [360/391] - Loss: 0.2758 - Train Accuracy: 0.8827\n",
      "Batch [370/391] - Loss: 0.1817 - Train Accuracy: 0.8831\n",
      "Batch [380/391] - Loss: 0.3226 - Train Accuracy: 0.8828\n",
      "Batch [390/391] - Loss: 0.2415 - Train Accuracy: 0.8832\n",
      "\n",
      "Validation Loss: 0.2925 - Validation Accuracy: 0.8906\n",
      "Epoch [4/20] - Train Loss: 0.2753 - Train Accuracy: 0.8832 - Val Loss: 0.2925 - Val Accuracy: 0.8906 - LR: 0.001000\n",
      "\n",
      "Epoch [5/20] - Training...\n",
      "Batch [10/391] - Loss: 0.1360 - Train Accuracy: 0.9219\n",
      "Batch [20/391] - Loss: 0.1697 - Train Accuracy: 0.9352\n",
      "Batch [30/391] - Loss: 0.1649 - Train Accuracy: 0.9245\n",
      "Batch [40/391] - Loss: 0.2000 - Train Accuracy: 0.9223\n",
      "Batch [50/391] - Loss: 0.1354 - Train Accuracy: 0.9213\n",
      "Batch [60/391] - Loss: 0.2045 - Train Accuracy: 0.9190\n",
      "Batch [70/391] - Loss: 0.1725 - Train Accuracy: 0.9187\n",
      "Batch [80/391] - Loss: 0.3051 - Train Accuracy: 0.9176\n",
      "Batch [90/391] - Loss: 0.2860 - Train Accuracy: 0.9163\n",
      "Batch [100/391] - Loss: 0.2121 - Train Accuracy: 0.9163\n",
      "Batch [110/391] - Loss: 0.1364 - Train Accuracy: 0.9156\n",
      "Batch [120/391] - Loss: 0.2140 - Train Accuracy: 0.9161\n",
      "Batch [130/391] - Loss: 0.1386 - Train Accuracy: 0.9162\n",
      "Batch [140/391] - Loss: 0.0733 - Train Accuracy: 0.9156\n",
      "Batch [150/391] - Loss: 0.1983 - Train Accuracy: 0.9150\n",
      "Batch [160/391] - Loss: 0.2787 - Train Accuracy: 0.9139\n",
      "Batch [170/391] - Loss: 0.2788 - Train Accuracy: 0.9142\n",
      "Batch [180/391] - Loss: 0.1369 - Train Accuracy: 0.9145\n",
      "Batch [190/391] - Loss: 0.2071 - Train Accuracy: 0.9150\n",
      "Batch [200/391] - Loss: 0.2017 - Train Accuracy: 0.9148\n",
      "Batch [210/391] - Loss: 0.1600 - Train Accuracy: 0.9167\n",
      "Batch [220/391] - Loss: 0.1150 - Train Accuracy: 0.9164\n",
      "Batch [230/391] - Loss: 0.3072 - Train Accuracy: 0.9158\n",
      "Batch [240/391] - Loss: 0.1937 - Train Accuracy: 0.9156\n",
      "Batch [250/391] - Loss: 0.1639 - Train Accuracy: 0.9154\n",
      "Batch [260/391] - Loss: 0.2237 - Train Accuracy: 0.9152\n",
      "Batch [270/391] - Loss: 0.2904 - Train Accuracy: 0.9150\n",
      "Batch [280/391] - Loss: 0.2459 - Train Accuracy: 0.9155\n",
      "Batch [290/391] - Loss: 0.1994 - Train Accuracy: 0.9155\n",
      "Batch [300/391] - Loss: 0.2565 - Train Accuracy: 0.9155\n",
      "Batch [310/391] - Loss: 0.0546 - Train Accuracy: 0.9158\n",
      "Batch [320/391] - Loss: 0.1893 - Train Accuracy: 0.9157\n",
      "Batch [330/391] - Loss: 0.1635 - Train Accuracy: 0.9157\n",
      "Batch [340/391] - Loss: 0.2987 - Train Accuracy: 0.9158\n",
      "Batch [350/391] - Loss: 0.2970 - Train Accuracy: 0.9161\n",
      "Batch [360/391] - Loss: 0.1730 - Train Accuracy: 0.9168\n",
      "Batch [370/391] - Loss: 0.2143 - Train Accuracy: 0.9169\n",
      "Batch [380/391] - Loss: 0.2484 - Train Accuracy: 0.9167\n",
      "Batch [390/391] - Loss: 0.1511 - Train Accuracy: 0.9169\n",
      "\n",
      "Validation Loss: 0.3152 - Validation Accuracy: 0.8632\n",
      "Epoch [5/20] - Train Loss: 0.2039 - Train Accuracy: 0.9168 - Val Loss: 0.3152 - Val Accuracy: 0.8632 - LR: 0.001000\n",
      "\n",
      "Epoch [6/20] - Training...\n",
      "Batch [10/391] - Loss: 0.2489 - Train Accuracy: 0.9469\n",
      "Batch [20/391] - Loss: 0.1057 - Train Accuracy: 0.9523\n",
      "Batch [30/391] - Loss: 0.1723 - Train Accuracy: 0.9526\n",
      "Batch [40/391] - Loss: 0.0918 - Train Accuracy: 0.9492\n",
      "Batch [50/391] - Loss: 0.1795 - Train Accuracy: 0.9472\n",
      "Batch [60/391] - Loss: 0.0941 - Train Accuracy: 0.9464\n",
      "Batch [70/391] - Loss: 0.1208 - Train Accuracy: 0.9478\n",
      "Batch [80/391] - Loss: 0.1187 - Train Accuracy: 0.9473\n",
      "Batch [90/391] - Loss: 0.1285 - Train Accuracy: 0.9474\n",
      "Batch [100/391] - Loss: 0.1344 - Train Accuracy: 0.9486\n",
      "Batch [110/391] - Loss: 0.1235 - Train Accuracy: 0.9489\n",
      "Batch [120/391] - Loss: 0.0640 - Train Accuracy: 0.9478\n",
      "Batch [130/391] - Loss: 0.0495 - Train Accuracy: 0.9472\n",
      "Batch [140/391] - Loss: 0.1624 - Train Accuracy: 0.9468\n",
      "Batch [150/391] - Loss: 0.2456 - Train Accuracy: 0.9464\n",
      "Batch [160/391] - Loss: 0.2020 - Train Accuracy: 0.9454\n",
      "Batch [170/391] - Loss: 0.0951 - Train Accuracy: 0.9449\n",
      "Batch [180/391] - Loss: 0.2187 - Train Accuracy: 0.9448\n",
      "Batch [190/391] - Loss: 0.1862 - Train Accuracy: 0.9446\n",
      "Batch [200/391] - Loss: 0.0882 - Train Accuracy: 0.9445\n",
      "Batch [210/391] - Loss: 0.1750 - Train Accuracy: 0.9447\n",
      "Batch [220/391] - Loss: 0.1173 - Train Accuracy: 0.9450\n",
      "Batch [230/391] - Loss: 0.1267 - Train Accuracy: 0.9455\n",
      "Batch [240/391] - Loss: 0.1864 - Train Accuracy: 0.9444\n",
      "Batch [250/391] - Loss: 0.1713 - Train Accuracy: 0.9438\n",
      "Batch [260/391] - Loss: 0.1921 - Train Accuracy: 0.9429\n",
      "Batch [270/391] - Loss: 0.1405 - Train Accuracy: 0.9427\n",
      "Batch [280/391] - Loss: 0.1437 - Train Accuracy: 0.9421\n",
      "Batch [290/391] - Loss: 0.1727 - Train Accuracy: 0.9420\n",
      "Batch [300/391] - Loss: 0.0826 - Train Accuracy: 0.9420\n",
      "Batch [310/391] - Loss: 0.0631 - Train Accuracy: 0.9412\n",
      "Batch [320/391] - Loss: 0.1790 - Train Accuracy: 0.9411\n",
      "Batch [330/391] - Loss: 0.1436 - Train Accuracy: 0.9408\n",
      "Batch [340/391] - Loss: 0.1932 - Train Accuracy: 0.9411\n",
      "Batch [350/391] - Loss: 0.1118 - Train Accuracy: 0.9412\n",
      "Batch [360/391] - Loss: 0.1273 - Train Accuracy: 0.9407\n",
      "Batch [370/391] - Loss: 0.1615 - Train Accuracy: 0.9406\n",
      "Batch [380/391] - Loss: 0.1271 - Train Accuracy: 0.9407\n",
      "Batch [390/391] - Loss: 0.2052 - Train Accuracy: 0.9401\n",
      "\n",
      "Validation Loss: 0.2319 - Validation Accuracy: 0.9104\n",
      "Epoch [6/20] - Train Loss: 0.1510 - Train Accuracy: 0.9402 - Val Loss: 0.2319 - Val Accuracy: 0.9104 - LR: 0.001000\n",
      "\n",
      "Epoch [7/20] - Training...\n",
      "Batch [10/391] - Loss: 0.1214 - Train Accuracy: 0.9578\n",
      "Batch [20/391] - Loss: 0.0743 - Train Accuracy: 0.9648\n",
      "Batch [30/391] - Loss: 0.2492 - Train Accuracy: 0.9635\n",
      "Batch [40/391] - Loss: 0.1656 - Train Accuracy: 0.9656\n",
      "Batch [50/391] - Loss: 0.0871 - Train Accuracy: 0.9666\n",
      "Batch [60/391] - Loss: 0.0610 - Train Accuracy: 0.9656\n",
      "Batch [70/391] - Loss: 0.1128 - Train Accuracy: 0.9629\n",
      "Batch [80/391] - Loss: 0.1637 - Train Accuracy: 0.9619\n",
      "Batch [90/391] - Loss: 0.0912 - Train Accuracy: 0.9611\n",
      "Batch [100/391] - Loss: 0.1837 - Train Accuracy: 0.9608\n",
      "Batch [110/391] - Loss: 0.0825 - Train Accuracy: 0.9602\n",
      "Batch [120/391] - Loss: 0.1807 - Train Accuracy: 0.9603\n",
      "Batch [130/391] - Loss: 0.1309 - Train Accuracy: 0.9597\n",
      "Batch [140/391] - Loss: 0.1561 - Train Accuracy: 0.9594\n",
      "Batch [150/391] - Loss: 0.0643 - Train Accuracy: 0.9586\n",
      "Batch [160/391] - Loss: 0.1504 - Train Accuracy: 0.9577\n",
      "Batch [170/391] - Loss: 0.0891 - Train Accuracy: 0.9564\n",
      "Batch [180/391] - Loss: 0.2020 - Train Accuracy: 0.9562\n",
      "Batch [190/391] - Loss: 0.0925 - Train Accuracy: 0.9566\n",
      "Batch [200/391] - Loss: 0.0733 - Train Accuracy: 0.9568\n",
      "Batch [210/391] - Loss: 0.1010 - Train Accuracy: 0.9564\n",
      "Batch [220/391] - Loss: 0.0734 - Train Accuracy: 0.9559\n",
      "Batch [230/391] - Loss: 0.0964 - Train Accuracy: 0.9562\n",
      "Batch [240/391] - Loss: 0.1132 - Train Accuracy: 0.9562\n",
      "Batch [250/391] - Loss: 0.0811 - Train Accuracy: 0.9558\n",
      "Batch [260/391] - Loss: 0.1455 - Train Accuracy: 0.9555\n",
      "Batch [270/391] - Loss: 0.2445 - Train Accuracy: 0.9550\n",
      "Batch [280/391] - Loss: 0.0788 - Train Accuracy: 0.9546\n",
      "Batch [290/391] - Loss: 0.0701 - Train Accuracy: 0.9550\n",
      "Batch [300/391] - Loss: 0.1290 - Train Accuracy: 0.9552\n",
      "Batch [310/391] - Loss: 0.1044 - Train Accuracy: 0.9549\n",
      "Batch [320/391] - Loss: 0.1289 - Train Accuracy: 0.9546\n",
      "Batch [330/391] - Loss: 0.1169 - Train Accuracy: 0.9545\n",
      "Batch [340/391] - Loss: 0.0944 - Train Accuracy: 0.9545\n",
      "Batch [350/391] - Loss: 0.1338 - Train Accuracy: 0.9544\n",
      "Batch [360/391] - Loss: 0.0569 - Train Accuracy: 0.9541\n",
      "Batch [370/391] - Loss: 0.0912 - Train Accuracy: 0.9540\n",
      "Batch [380/391] - Loss: 0.1205 - Train Accuracy: 0.9535\n",
      "Batch [390/391] - Loss: 0.2780 - Train Accuracy: 0.9534\n",
      "\n",
      "Validation Loss: 0.2102 - Validation Accuracy: 0.9162\n",
      "Epoch [7/20] - Train Loss: 0.1202 - Train Accuracy: 0.9535 - Val Loss: 0.2102 - Val Accuracy: 0.9162 - LR: 0.001000\n",
      "\n",
      "Epoch [8/20] - Training...\n",
      "Batch [10/391] - Loss: 0.2185 - Train Accuracy: 0.9625\n",
      "Batch [20/391] - Loss: 0.0568 - Train Accuracy: 0.9648\n",
      "Batch [30/391] - Loss: 0.1100 - Train Accuracy: 0.9656\n",
      "Batch [40/391] - Loss: 0.0375 - Train Accuracy: 0.9676\n",
      "Batch [50/391] - Loss: 0.0487 - Train Accuracy: 0.9666\n",
      "Batch [60/391] - Loss: 0.0805 - Train Accuracy: 0.9672\n",
      "Batch [70/391] - Loss: 0.0284 - Train Accuracy: 0.9685\n",
      "Batch [80/391] - Loss: 0.1136 - Train Accuracy: 0.9686\n",
      "Batch [90/391] - Loss: 0.0721 - Train Accuracy: 0.9691\n",
      "Batch [100/391] - Loss: 0.0152 - Train Accuracy: 0.9681\n",
      "Batch [110/391] - Loss: 0.0322 - Train Accuracy: 0.9676\n",
      "Batch [120/391] - Loss: 0.0444 - Train Accuracy: 0.9682\n",
      "Batch [130/391] - Loss: 0.0942 - Train Accuracy: 0.9678\n",
      "Batch [140/391] - Loss: 0.0522 - Train Accuracy: 0.9679\n",
      "Batch [150/391] - Loss: 0.0357 - Train Accuracy: 0.9682\n",
      "Batch [160/391] - Loss: 0.0620 - Train Accuracy: 0.9686\n",
      "Batch [170/391] - Loss: 0.0438 - Train Accuracy: 0.9695\n",
      "Batch [180/391] - Loss: 0.0585 - Train Accuracy: 0.9697\n",
      "Batch [190/391] - Loss: 0.0290 - Train Accuracy: 0.9700\n",
      "Batch [200/391] - Loss: 0.0747 - Train Accuracy: 0.9698\n",
      "Batch [210/391] - Loss: 0.0865 - Train Accuracy: 0.9693\n",
      "Batch [220/391] - Loss: 0.0606 - Train Accuracy: 0.9683\n",
      "Batch [230/391] - Loss: 0.0333 - Train Accuracy: 0.9681\n",
      "Batch [240/391] - Loss: 0.0518 - Train Accuracy: 0.9675\n",
      "Batch [250/391] - Loss: 0.0742 - Train Accuracy: 0.9673\n",
      "Batch [260/391] - Loss: 0.0404 - Train Accuracy: 0.9668\n",
      "Batch [270/391] - Loss: 0.2303 - Train Accuracy: 0.9664\n",
      "Batch [280/391] - Loss: 0.0180 - Train Accuracy: 0.9666\n",
      "Batch [290/391] - Loss: 0.0530 - Train Accuracy: 0.9666\n",
      "Batch [300/391] - Loss: 0.0286 - Train Accuracy: 0.9667\n",
      "Batch [310/391] - Loss: 0.0382 - Train Accuracy: 0.9668\n",
      "Batch [320/391] - Loss: 0.1116 - Train Accuracy: 0.9661\n",
      "Batch [330/391] - Loss: 0.0502 - Train Accuracy: 0.9662\n",
      "Batch [340/391] - Loss: 0.0704 - Train Accuracy: 0.9663\n",
      "Batch [350/391] - Loss: 0.0357 - Train Accuracy: 0.9662\n",
      "Batch [360/391] - Loss: 0.0610 - Train Accuracy: 0.9657\n",
      "Batch [370/391] - Loss: 0.1978 - Train Accuracy: 0.9657\n",
      "Batch [380/391] - Loss: 0.0743 - Train Accuracy: 0.9655\n",
      "Batch [390/391] - Loss: 0.2299 - Train Accuracy: 0.9649\n",
      "\n",
      "Validation Loss: 0.2529 - Validation Accuracy: 0.8900\n",
      "Epoch [8/20] - Train Loss: 0.0864 - Train Accuracy: 0.9649 - Val Loss: 0.2529 - Val Accuracy: 0.8900 - LR: 0.001000\n",
      "\n",
      "Epoch [9/20] - Training...\n",
      "Batch [10/391] - Loss: 0.1643 - Train Accuracy: 0.9703\n",
      "Batch [20/391] - Loss: 0.0194 - Train Accuracy: 0.9727\n",
      "Batch [30/391] - Loss: 0.0627 - Train Accuracy: 0.9734\n",
      "Batch [40/391] - Loss: 0.0991 - Train Accuracy: 0.9734\n",
      "Batch [50/391] - Loss: 0.0199 - Train Accuracy: 0.9747\n",
      "Batch [60/391] - Loss: 0.0727 - Train Accuracy: 0.9760\n",
      "Batch [70/391] - Loss: 0.0362 - Train Accuracy: 0.9754\n",
      "Batch [80/391] - Loss: 0.1063 - Train Accuracy: 0.9771\n",
      "Batch [90/391] - Loss: 0.1518 - Train Accuracy: 0.9764\n",
      "Batch [100/391] - Loss: 0.0669 - Train Accuracy: 0.9761\n",
      "Batch [110/391] - Loss: 0.1365 - Train Accuracy: 0.9749\n",
      "Batch [120/391] - Loss: 0.0498 - Train Accuracy: 0.9751\n",
      "Batch [130/391] - Loss: 0.0500 - Train Accuracy: 0.9746\n",
      "Batch [140/391] - Loss: 0.0864 - Train Accuracy: 0.9746\n",
      "Batch [150/391] - Loss: 0.0270 - Train Accuracy: 0.9749\n",
      "Batch [160/391] - Loss: 0.0745 - Train Accuracy: 0.9746\n",
      "Batch [170/391] - Loss: 0.0580 - Train Accuracy: 0.9744\n",
      "Batch [180/391] - Loss: 0.0270 - Train Accuracy: 0.9740\n",
      "Batch [190/391] - Loss: 0.0732 - Train Accuracy: 0.9738\n",
      "Batch [200/391] - Loss: 0.0454 - Train Accuracy: 0.9734\n",
      "Batch [210/391] - Loss: 0.0571 - Train Accuracy: 0.9734\n",
      "Batch [220/391] - Loss: 0.0229 - Train Accuracy: 0.9734\n",
      "Batch [230/391] - Loss: 0.0378 - Train Accuracy: 0.9735\n",
      "Batch [240/391] - Loss: 0.0150 - Train Accuracy: 0.9736\n",
      "Batch [250/391] - Loss: 0.1260 - Train Accuracy: 0.9732\n",
      "Batch [260/391] - Loss: 0.0958 - Train Accuracy: 0.9726\n",
      "Batch [270/391] - Loss: 0.0642 - Train Accuracy: 0.9725\n",
      "Batch [280/391] - Loss: 0.0940 - Train Accuracy: 0.9718\n",
      "Batch [290/391] - Loss: 0.0279 - Train Accuracy: 0.9716\n",
      "Batch [300/391] - Loss: 0.0634 - Train Accuracy: 0.9712\n",
      "Batch [310/391] - Loss: 0.0675 - Train Accuracy: 0.9709\n",
      "Batch [320/391] - Loss: 0.0423 - Train Accuracy: 0.9710\n",
      "Batch [330/391] - Loss: 0.0414 - Train Accuracy: 0.9713\n",
      "Batch [340/391] - Loss: 0.0737 - Train Accuracy: 0.9712\n",
      "Batch [350/391] - Loss: 0.0945 - Train Accuracy: 0.9717\n",
      "Batch [360/391] - Loss: 0.0705 - Train Accuracy: 0.9716\n",
      "Batch [370/391] - Loss: 0.1751 - Train Accuracy: 0.9715\n",
      "Batch [380/391] - Loss: 0.1186 - Train Accuracy: 0.9716\n",
      "Batch [390/391] - Loss: 0.0926 - Train Accuracy: 0.9716\n",
      "\n",
      "Validation Loss: 0.2119 - Validation Accuracy: 0.9138\n",
      "Epoch [9/20] - Train Loss: 0.0739 - Train Accuracy: 0.9716 - Val Loss: 0.2119 - Val Accuracy: 0.9138 - LR: 0.001000\n",
      "\n",
      "Epoch [10/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0639 - Train Accuracy: 0.9797\n",
      "Batch [20/391] - Loss: 0.0579 - Train Accuracy: 0.9758\n",
      "Batch [30/391] - Loss: 0.0531 - Train Accuracy: 0.9760\n",
      "Batch [40/391] - Loss: 0.0576 - Train Accuracy: 0.9750\n",
      "Batch [50/391] - Loss: 0.0290 - Train Accuracy: 0.9769\n",
      "Batch [60/391] - Loss: 0.0746 - Train Accuracy: 0.9773\n",
      "Batch [70/391] - Loss: 0.0204 - Train Accuracy: 0.9770\n",
      "Batch [80/391] - Loss: 0.0199 - Train Accuracy: 0.9773\n",
      "Batch [90/391] - Loss: 0.0230 - Train Accuracy: 0.9760\n",
      "Batch [100/391] - Loss: 0.0550 - Train Accuracy: 0.9766\n",
      "Batch [110/391] - Loss: 0.0214 - Train Accuracy: 0.9768\n",
      "Batch [120/391] - Loss: 0.0344 - Train Accuracy: 0.9773\n",
      "Batch [130/391] - Loss: 0.0273 - Train Accuracy: 0.9768\n",
      "Batch [140/391] - Loss: 0.0796 - Train Accuracy: 0.9768\n",
      "Batch [150/391] - Loss: 0.0974 - Train Accuracy: 0.9769\n",
      "Batch [160/391] - Loss: 0.2521 - Train Accuracy: 0.9768\n",
      "Batch [170/391] - Loss: 0.1404 - Train Accuracy: 0.9769\n",
      "Batch [180/391] - Loss: 0.1543 - Train Accuracy: 0.9761\n",
      "Batch [190/391] - Loss: 0.1104 - Train Accuracy: 0.9755\n",
      "Batch [200/391] - Loss: 0.1169 - Train Accuracy: 0.9754\n",
      "Batch [210/391] - Loss: 0.0807 - Train Accuracy: 0.9753\n",
      "Batch [220/391] - Loss: 0.0295 - Train Accuracy: 0.9754\n",
      "Batch [230/391] - Loss: 0.0419 - Train Accuracy: 0.9755\n",
      "Batch [240/391] - Loss: 0.1449 - Train Accuracy: 0.9751\n",
      "Batch [250/391] - Loss: 0.0513 - Train Accuracy: 0.9756\n",
      "Batch [260/391] - Loss: 0.1351 - Train Accuracy: 0.9757\n",
      "Batch [270/391] - Loss: 0.0894 - Train Accuracy: 0.9756\n",
      "Batch [280/391] - Loss: 0.0514 - Train Accuracy: 0.9760\n",
      "Batch [290/391] - Loss: 0.1281 - Train Accuracy: 0.9760\n",
      "Batch [300/391] - Loss: 0.0481 - Train Accuracy: 0.9756\n",
      "Batch [310/391] - Loss: 0.0422 - Train Accuracy: 0.9753\n",
      "Batch [320/391] - Loss: 0.0471 - Train Accuracy: 0.9750\n",
      "Batch [330/391] - Loss: 0.0353 - Train Accuracy: 0.9750\n",
      "Batch [340/391] - Loss: 0.2438 - Train Accuracy: 0.9746\n",
      "Batch [350/391] - Loss: 0.0377 - Train Accuracy: 0.9745\n",
      "Batch [360/391] - Loss: 0.1457 - Train Accuracy: 0.9746\n",
      "Batch [370/391] - Loss: 0.0169 - Train Accuracy: 0.9744\n",
      "Batch [380/391] - Loss: 0.0869 - Train Accuracy: 0.9743\n",
      "Batch [390/391] - Loss: 0.1553 - Train Accuracy: 0.9746\n",
      "\n",
      "Validation Loss: 0.2525 - Validation Accuracy: 0.9182\n",
      "Epoch [10/20] - Train Loss: 0.0643 - Train Accuracy: 0.9746 - Val Loss: 0.2525 - Val Accuracy: 0.9182 - LR: 0.001000\n",
      "\n",
      "Epoch [11/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0423 - Train Accuracy: 0.9844\n",
      "Batch [20/391] - Loss: 0.0150 - Train Accuracy: 0.9844\n",
      "Batch [30/391] - Loss: 0.0602 - Train Accuracy: 0.9823\n",
      "Batch [40/391] - Loss: 0.0281 - Train Accuracy: 0.9836\n",
      "Batch [50/391] - Loss: 0.0046 - Train Accuracy: 0.9841\n",
      "Batch [60/391] - Loss: 0.0062 - Train Accuracy: 0.9841\n",
      "Batch [70/391] - Loss: 0.1203 - Train Accuracy: 0.9853\n",
      "Batch [80/391] - Loss: 0.0120 - Train Accuracy: 0.9852\n",
      "Batch [90/391] - Loss: 0.0073 - Train Accuracy: 0.9861\n",
      "Batch [100/391] - Loss: 0.0100 - Train Accuracy: 0.9847\n",
      "Batch [110/391] - Loss: 0.0147 - Train Accuracy: 0.9852\n",
      "Batch [120/391] - Loss: 0.1200 - Train Accuracy: 0.9842\n",
      "Batch [130/391] - Loss: 0.0244 - Train Accuracy: 0.9840\n",
      "Batch [140/391] - Loss: 0.0194 - Train Accuracy: 0.9839\n",
      "Batch [150/391] - Loss: 0.0274 - Train Accuracy: 0.9839\n",
      "Batch [160/391] - Loss: 0.1127 - Train Accuracy: 0.9835\n",
      "Batch [170/391] - Loss: 0.0306 - Train Accuracy: 0.9835\n",
      "Batch [180/391] - Loss: 0.0350 - Train Accuracy: 0.9841\n",
      "Batch [190/391] - Loss: 0.1302 - Train Accuracy: 0.9842\n",
      "Batch [200/391] - Loss: 0.0116 - Train Accuracy: 0.9843\n",
      "Batch [210/391] - Loss: 0.0366 - Train Accuracy: 0.9839\n",
      "Batch [220/391] - Loss: 0.0425 - Train Accuracy: 0.9835\n",
      "Batch [230/391] - Loss: 0.0051 - Train Accuracy: 0.9834\n",
      "Batch [240/391] - Loss: 0.0975 - Train Accuracy: 0.9833\n",
      "Batch [250/391] - Loss: 0.0061 - Train Accuracy: 0.9832\n",
      "Batch [260/391] - Loss: 0.0311 - Train Accuracy: 0.9834\n",
      "Batch [270/391] - Loss: 0.0294 - Train Accuracy: 0.9832\n",
      "Batch [280/391] - Loss: 0.0121 - Train Accuracy: 0.9835\n",
      "Batch [290/391] - Loss: 0.0061 - Train Accuracy: 0.9836\n",
      "Batch [300/391] - Loss: 0.0475 - Train Accuracy: 0.9836\n",
      "Batch [310/391] - Loss: 0.0211 - Train Accuracy: 0.9838\n",
      "Batch [320/391] - Loss: 0.0202 - Train Accuracy: 0.9837\n",
      "Batch [330/391] - Loss: 0.1858 - Train Accuracy: 0.9832\n",
      "Batch [340/391] - Loss: 0.0214 - Train Accuracy: 0.9830\n",
      "Batch [350/391] - Loss: 0.1021 - Train Accuracy: 0.9827\n",
      "Batch [360/391] - Loss: 0.0884 - Train Accuracy: 0.9821\n",
      "Batch [370/391] - Loss: 0.1250 - Train Accuracy: 0.9816\n",
      "Batch [380/391] - Loss: 0.0870 - Train Accuracy: 0.9815\n",
      "Batch [390/391] - Loss: 0.1249 - Train Accuracy: 0.9813\n",
      "\n",
      "Validation Loss: 0.2250 - Validation Accuracy: 0.9176\n",
      "Epoch [11/20] - Train Loss: 0.0486 - Train Accuracy: 0.9813 - Val Loss: 0.2250 - Val Accuracy: 0.9176 - LR: 0.001000\n",
      "\n",
      "Epoch [12/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0054 - Train Accuracy: 0.9938\n",
      "Batch [20/391] - Loss: 0.0444 - Train Accuracy: 0.9938\n",
      "Batch [30/391] - Loss: 0.0713 - Train Accuracy: 0.9906\n",
      "Batch [40/391] - Loss: 0.0654 - Train Accuracy: 0.9891\n",
      "Batch [50/391] - Loss: 0.0791 - Train Accuracy: 0.9878\n",
      "Batch [60/391] - Loss: 0.0434 - Train Accuracy: 0.9872\n",
      "Batch [70/391] - Loss: 0.0330 - Train Accuracy: 0.9871\n",
      "Batch [80/391] - Loss: 0.0139 - Train Accuracy: 0.9879\n",
      "Batch [90/391] - Loss: 0.0080 - Train Accuracy: 0.9880\n",
      "Batch [100/391] - Loss: 0.0527 - Train Accuracy: 0.9886\n",
      "Batch [110/391] - Loss: 0.0036 - Train Accuracy: 0.9893\n",
      "Batch [120/391] - Loss: 0.0395 - Train Accuracy: 0.9897\n",
      "Batch [130/391] - Loss: 0.0038 - Train Accuracy: 0.9900\n",
      "Batch [140/391] - Loss: 0.0212 - Train Accuracy: 0.9898\n",
      "Batch [150/391] - Loss: 0.0166 - Train Accuracy: 0.9897\n",
      "Batch [160/391] - Loss: 0.0282 - Train Accuracy: 0.9900\n",
      "Batch [170/391] - Loss: 0.0125 - Train Accuracy: 0.9905\n",
      "Batch [180/391] - Loss: 0.0197 - Train Accuracy: 0.9909\n",
      "Batch [190/391] - Loss: 0.0865 - Train Accuracy: 0.9908\n",
      "Batch [200/391] - Loss: 0.0216 - Train Accuracy: 0.9908\n",
      "Batch [210/391] - Loss: 0.0447 - Train Accuracy: 0.9908\n",
      "Batch [220/391] - Loss: 0.0195 - Train Accuracy: 0.9906\n",
      "Batch [230/391] - Loss: 0.0077 - Train Accuracy: 0.9905\n",
      "Batch [240/391] - Loss: 0.0248 - Train Accuracy: 0.9903\n",
      "Batch [250/391] - Loss: 0.0341 - Train Accuracy: 0.9904\n",
      "Batch [260/391] - Loss: 0.0338 - Train Accuracy: 0.9903\n",
      "Batch [270/391] - Loss: 0.0030 - Train Accuracy: 0.9905\n",
      "Batch [280/391] - Loss: 0.0353 - Train Accuracy: 0.9903\n",
      "Batch [290/391] - Loss: 0.0051 - Train Accuracy: 0.9903\n",
      "Batch [300/391] - Loss: 0.0025 - Train Accuracy: 0.9905\n",
      "Batch [310/391] - Loss: 0.0064 - Train Accuracy: 0.9907\n",
      "Batch [320/391] - Loss: 0.0198 - Train Accuracy: 0.9909\n",
      "Batch [330/391] - Loss: 0.0093 - Train Accuracy: 0.9910\n",
      "Batch [340/391] - Loss: 0.0288 - Train Accuracy: 0.9911\n",
      "Batch [350/391] - Loss: 0.0035 - Train Accuracy: 0.9912\n",
      "Batch [360/391] - Loss: 0.0088 - Train Accuracy: 0.9913\n",
      "Batch [370/391] - Loss: 0.0173 - Train Accuracy: 0.9913\n",
      "Batch [380/391] - Loss: 0.0234 - Train Accuracy: 0.9913\n",
      "Batch [390/391] - Loss: 0.0087 - Train Accuracy: 0.9913\n",
      "\n",
      "Validation Loss: 0.2525 - Validation Accuracy: 0.9264\n",
      "Epoch [12/20] - Train Loss: 0.0229 - Train Accuracy: 0.9913 - Val Loss: 0.2525 - Val Accuracy: 0.9264 - LR: 0.000200\n",
      "\n",
      "Epoch [13/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0014 - Train Accuracy: 0.9984\n",
      "Batch [20/391] - Loss: 0.0237 - Train Accuracy: 0.9961\n",
      "Batch [30/391] - Loss: 0.0008 - Train Accuracy: 0.9969\n",
      "Batch [40/391] - Loss: 0.0141 - Train Accuracy: 0.9969\n",
      "Batch [50/391] - Loss: 0.0004 - Train Accuracy: 0.9972\n",
      "Batch [60/391] - Loss: 0.0119 - Train Accuracy: 0.9971\n",
      "Batch [70/391] - Loss: 0.0098 - Train Accuracy: 0.9969\n",
      "Batch [80/391] - Loss: 0.0014 - Train Accuracy: 0.9965\n",
      "Batch [90/391] - Loss: 0.0015 - Train Accuracy: 0.9964\n",
      "Batch [100/391] - Loss: 0.0030 - Train Accuracy: 0.9961\n",
      "Batch [110/391] - Loss: 0.0022 - Train Accuracy: 0.9964\n",
      "Batch [120/391] - Loss: 0.0042 - Train Accuracy: 0.9962\n",
      "Batch [130/391] - Loss: 0.0229 - Train Accuracy: 0.9960\n",
      "Batch [140/391] - Loss: 0.0264 - Train Accuracy: 0.9959\n",
      "Batch [150/391] - Loss: 0.0047 - Train Accuracy: 0.9955\n",
      "Batch [160/391] - Loss: 0.0065 - Train Accuracy: 0.9956\n",
      "Batch [170/391] - Loss: 0.0189 - Train Accuracy: 0.9957\n",
      "Batch [180/391] - Loss: 0.0047 - Train Accuracy: 0.9958\n",
      "Batch [190/391] - Loss: 0.0016 - Train Accuracy: 0.9959\n",
      "Batch [200/391] - Loss: 0.0059 - Train Accuracy: 0.9959\n",
      "Batch [210/391] - Loss: 0.0004 - Train Accuracy: 0.9960\n",
      "Batch [220/391] - Loss: 0.0047 - Train Accuracy: 0.9960\n",
      "Batch [230/391] - Loss: 0.0012 - Train Accuracy: 0.9958\n",
      "Batch [240/391] - Loss: 0.0307 - Train Accuracy: 0.9956\n",
      "Batch [250/391] - Loss: 0.0252 - Train Accuracy: 0.9956\n",
      "Batch [260/391] - Loss: 0.0039 - Train Accuracy: 0.9954\n",
      "Batch [270/391] - Loss: 0.0452 - Train Accuracy: 0.9953\n",
      "Batch [280/391] - Loss: 0.0287 - Train Accuracy: 0.9951\n",
      "Batch [290/391] - Loss: 0.0065 - Train Accuracy: 0.9952\n",
      "Batch [300/391] - Loss: 0.0035 - Train Accuracy: 0.9952\n",
      "Batch [310/391] - Loss: 0.0007 - Train Accuracy: 0.9953\n",
      "Batch [320/391] - Loss: 0.0054 - Train Accuracy: 0.9953\n",
      "Batch [330/391] - Loss: 0.0218 - Train Accuracy: 0.9954\n",
      "Batch [340/391] - Loss: 0.0124 - Train Accuracy: 0.9955\n",
      "Batch [350/391] - Loss: 0.0085 - Train Accuracy: 0.9956\n",
      "Batch [360/391] - Loss: 0.0210 - Train Accuracy: 0.9956\n",
      "Batch [370/391] - Loss: 0.0075 - Train Accuracy: 0.9956\n",
      "Batch [380/391] - Loss: 0.0251 - Train Accuracy: 0.9954\n",
      "Batch [390/391] - Loss: 0.0090 - Train Accuracy: 0.9955\n",
      "\n",
      "Validation Loss: 0.3183 - Validation Accuracy: 0.9254\n",
      "Epoch [13/20] - Train Loss: 0.0118 - Train Accuracy: 0.9955 - Val Loss: 0.3183 - Val Accuracy: 0.9254 - LR: 0.000200\n",
      "\n",
      "Epoch [14/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0148 - Train Accuracy: 0.9953\n",
      "Batch [20/391] - Loss: 0.0253 - Train Accuracy: 0.9969\n",
      "Batch [30/391] - Loss: 0.0035 - Train Accuracy: 0.9979\n",
      "Batch [40/391] - Loss: 0.0118 - Train Accuracy: 0.9973\n",
      "Batch [50/391] - Loss: 0.0054 - Train Accuracy: 0.9975\n",
      "Batch [60/391] - Loss: 0.0034 - Train Accuracy: 0.9979\n",
      "Batch [70/391] - Loss: 0.0096 - Train Accuracy: 0.9975\n",
      "Batch [80/391] - Loss: 0.0113 - Train Accuracy: 0.9975\n",
      "Batch [90/391] - Loss: 0.0011 - Train Accuracy: 0.9972\n",
      "Batch [100/391] - Loss: 0.0047 - Train Accuracy: 0.9973\n",
      "Batch [110/391] - Loss: 0.0019 - Train Accuracy: 0.9967\n",
      "Batch [120/391] - Loss: 0.0018 - Train Accuracy: 0.9966\n",
      "Batch [130/391] - Loss: 0.0016 - Train Accuracy: 0.9965\n",
      "Batch [140/391] - Loss: 0.0551 - Train Accuracy: 0.9963\n",
      "Batch [150/391] - Loss: 0.0011 - Train Accuracy: 0.9962\n",
      "Batch [160/391] - Loss: 0.0116 - Train Accuracy: 0.9963\n",
      "Batch [170/391] - Loss: 0.0032 - Train Accuracy: 0.9963\n",
      "Batch [180/391] - Loss: 0.0043 - Train Accuracy: 0.9964\n",
      "Batch [190/391] - Loss: 0.0043 - Train Accuracy: 0.9965\n",
      "Batch [200/391] - Loss: 0.0008 - Train Accuracy: 0.9963\n",
      "Batch [210/391] - Loss: 0.0037 - Train Accuracy: 0.9964\n",
      "Batch [220/391] - Loss: 0.0001 - Train Accuracy: 0.9961\n",
      "Batch [230/391] - Loss: 0.0125 - Train Accuracy: 0.9962\n",
      "Batch [240/391] - Loss: 0.0048 - Train Accuracy: 0.9962\n",
      "Batch [250/391] - Loss: 0.0033 - Train Accuracy: 0.9962\n",
      "Batch [260/391] - Loss: 0.0039 - Train Accuracy: 0.9963\n",
      "Batch [270/391] - Loss: 0.0011 - Train Accuracy: 0.9962\n",
      "Batch [280/391] - Loss: 0.0002 - Train Accuracy: 0.9963\n",
      "Batch [290/391] - Loss: 0.0059 - Train Accuracy: 0.9963\n",
      "Batch [300/391] - Loss: 0.0883 - Train Accuracy: 0.9963\n",
      "Batch [310/391] - Loss: 0.0015 - Train Accuracy: 0.9962\n",
      "Batch [320/391] - Loss: 0.0377 - Train Accuracy: 0.9962\n",
      "Batch [330/391] - Loss: 0.0121 - Train Accuracy: 0.9962\n",
      "Batch [340/391] - Loss: 0.0151 - Train Accuracy: 0.9962\n",
      "Batch [350/391] - Loss: 0.0675 - Train Accuracy: 0.9961\n",
      "Batch [360/391] - Loss: 0.0005 - Train Accuracy: 0.9961\n",
      "Batch [370/391] - Loss: 0.0002 - Train Accuracy: 0.9961\n",
      "Batch [380/391] - Loss: 0.0047 - Train Accuracy: 0.9961\n",
      "Batch [390/391] - Loss: 0.0001 - Train Accuracy: 0.9962\n",
      "\n",
      "Validation Loss: 0.3098 - Validation Accuracy: 0.9268\n",
      "Epoch [14/20] - Train Loss: 0.0094 - Train Accuracy: 0.9962 - Val Loss: 0.3098 - Val Accuracy: 0.9268 - LR: 0.000200\n",
      "\n",
      "Epoch [15/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0034 - Train Accuracy: 0.9953\n",
      "Batch [20/391] - Loss: 0.0037 - Train Accuracy: 0.9945\n",
      "Batch [30/391] - Loss: 0.0022 - Train Accuracy: 0.9938\n",
      "Batch [40/391] - Loss: 0.0090 - Train Accuracy: 0.9945\n",
      "Batch [50/391] - Loss: 0.0008 - Train Accuracy: 0.9956\n",
      "Batch [60/391] - Loss: 0.0002 - Train Accuracy: 0.9958\n",
      "Batch [70/391] - Loss: 0.0023 - Train Accuracy: 0.9964\n",
      "Batch [80/391] - Loss: 0.0006 - Train Accuracy: 0.9967\n",
      "Batch [90/391] - Loss: 0.0175 - Train Accuracy: 0.9969\n",
      "Batch [100/391] - Loss: 0.0082 - Train Accuracy: 0.9972\n",
      "Batch [110/391] - Loss: 0.0046 - Train Accuracy: 0.9973\n",
      "Batch [120/391] - Loss: 0.0056 - Train Accuracy: 0.9974\n",
      "Batch [130/391] - Loss: 0.0013 - Train Accuracy: 0.9975\n",
      "Batch [140/391] - Loss: 0.0028 - Train Accuracy: 0.9975\n",
      "Batch [150/391] - Loss: 0.0102 - Train Accuracy: 0.9976\n",
      "Batch [160/391] - Loss: 0.0003 - Train Accuracy: 0.9976\n",
      "Batch [170/391] - Loss: 0.0001 - Train Accuracy: 0.9975\n",
      "Batch [180/391] - Loss: 0.0027 - Train Accuracy: 0.9976\n",
      "Batch [190/391] - Loss: 0.0001 - Train Accuracy: 0.9975\n",
      "Batch [200/391] - Loss: 0.0010 - Train Accuracy: 0.9973\n",
      "Batch [210/391] - Loss: 0.0008 - Train Accuracy: 0.9972\n",
      "Batch [220/391] - Loss: 0.0033 - Train Accuracy: 0.9973\n",
      "Batch [230/391] - Loss: 0.0000 - Train Accuracy: 0.9974\n",
      "Batch [240/391] - Loss: 0.0047 - Train Accuracy: 0.9974\n",
      "Batch [250/391] - Loss: 0.0034 - Train Accuracy: 0.9972\n",
      "Batch [260/391] - Loss: 0.0061 - Train Accuracy: 0.9974\n",
      "Batch [270/391] - Loss: 0.0002 - Train Accuracy: 0.9972\n",
      "Batch [280/391] - Loss: 0.0003 - Train Accuracy: 0.9972\n",
      "Batch [290/391] - Loss: 0.0001 - Train Accuracy: 0.9973\n",
      "Batch [300/391] - Loss: 0.0006 - Train Accuracy: 0.9972\n",
      "Batch [310/391] - Loss: 0.0408 - Train Accuracy: 0.9971\n",
      "Batch [320/391] - Loss: 0.0114 - Train Accuracy: 0.9971\n",
      "Batch [330/391] - Loss: 0.0065 - Train Accuracy: 0.9972\n",
      "Batch [340/391] - Loss: 0.0002 - Train Accuracy: 0.9973\n",
      "Batch [350/391] - Loss: 0.0174 - Train Accuracy: 0.9973\n",
      "Batch [360/391] - Loss: 0.0004 - Train Accuracy: 0.9973\n",
      "Batch [370/391] - Loss: 0.0006 - Train Accuracy: 0.9973\n",
      "Batch [380/391] - Loss: 0.0259 - Train Accuracy: 0.9971\n",
      "Batch [390/391] - Loss: 0.0005 - Train Accuracy: 0.9971\n",
      "\n",
      "Validation Loss: 0.3777 - Validation Accuracy: 0.9260\n",
      "Epoch [15/20] - Train Loss: 0.0076 - Train Accuracy: 0.9971 - Val Loss: 0.3777 - Val Accuracy: 0.9260 - LR: 0.000200\n",
      "\n",
      "Epoch [16/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0057 - Train Accuracy: 0.9984\n",
      "Batch [20/391] - Loss: 0.0004 - Train Accuracy: 0.9969\n",
      "Batch [30/391] - Loss: 0.0038 - Train Accuracy: 0.9979\n",
      "Batch [40/391] - Loss: 0.0027 - Train Accuracy: 0.9977\n",
      "Batch [50/391] - Loss: 0.0003 - Train Accuracy: 0.9978\n",
      "Batch [60/391] - Loss: 0.0007 - Train Accuracy: 0.9979\n",
      "Batch [70/391] - Loss: 0.0003 - Train Accuracy: 0.9980\n",
      "Batch [80/391] - Loss: 0.0255 - Train Accuracy: 0.9975\n",
      "Batch [90/391] - Loss: 0.0039 - Train Accuracy: 0.9974\n",
      "Batch [100/391] - Loss: 0.0003 - Train Accuracy: 0.9973\n",
      "Batch [110/391] - Loss: 0.0005 - Train Accuracy: 0.9974\n",
      "Batch [120/391] - Loss: 0.0007 - Train Accuracy: 0.9975\n",
      "Batch [130/391] - Loss: 0.0001 - Train Accuracy: 0.9977\n",
      "Batch [140/391] - Loss: 0.0005 - Train Accuracy: 0.9978\n",
      "Batch [150/391] - Loss: 0.0115 - Train Accuracy: 0.9978\n",
      "Batch [160/391] - Loss: 0.0205 - Train Accuracy: 0.9978\n",
      "Batch [170/391] - Loss: 0.0000 - Train Accuracy: 0.9977\n",
      "Batch [180/391] - Loss: 0.0001 - Train Accuracy: 0.9978\n",
      "Batch [190/391] - Loss: 0.0005 - Train Accuracy: 0.9978\n",
      "Batch [200/391] - Loss: 0.0006 - Train Accuracy: 0.9977\n",
      "Batch [210/391] - Loss: 0.0002 - Train Accuracy: 0.9977\n",
      "Batch [220/391] - Loss: 0.0001 - Train Accuracy: 0.9977\n",
      "Batch [230/391] - Loss: 0.0001 - Train Accuracy: 0.9976\n",
      "Batch [240/391] - Loss: 0.0179 - Train Accuracy: 0.9976\n",
      "Batch [250/391] - Loss: 0.0470 - Train Accuracy: 0.9975\n",
      "Batch [260/391] - Loss: 0.0141 - Train Accuracy: 0.9973\n",
      "Batch [270/391] - Loss: 0.0052 - Train Accuracy: 0.9973\n",
      "Batch [280/391] - Loss: 0.0082 - Train Accuracy: 0.9972\n",
      "Batch [290/391] - Loss: 0.0005 - Train Accuracy: 0.9972\n",
      "Batch [300/391] - Loss: 0.0002 - Train Accuracy: 0.9973\n",
      "Batch [310/391] - Loss: 0.0023 - Train Accuracy: 0.9973\n",
      "Batch [320/391] - Loss: 0.0031 - Train Accuracy: 0.9973\n",
      "Batch [330/391] - Loss: 0.0063 - Train Accuracy: 0.9973\n",
      "Batch [340/391] - Loss: 0.0001 - Train Accuracy: 0.9972\n",
      "Batch [350/391] - Loss: 0.0012 - Train Accuracy: 0.9973\n",
      "Batch [360/391] - Loss: 0.0043 - Train Accuracy: 0.9973\n",
      "Batch [370/391] - Loss: 0.0033 - Train Accuracy: 0.9973\n",
      "Batch [380/391] - Loss: 0.0028 - Train Accuracy: 0.9973\n",
      "Batch [390/391] - Loss: 0.0036 - Train Accuracy: 0.9973\n",
      "\n",
      "Validation Loss: 0.3604 - Validation Accuracy: 0.9260\n",
      "Epoch [16/20] - Train Loss: 0.0071 - Train Accuracy: 0.9973 - Val Loss: 0.3604 - Val Accuracy: 0.9260 - LR: 0.000040\n",
      "\n",
      "Epoch [17/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0183 - Train Accuracy: 0.9969\n",
      "Batch [20/391] - Loss: 0.0002 - Train Accuracy: 0.9984\n",
      "Batch [30/391] - Loss: 0.0002 - Train Accuracy: 0.9984\n",
      "Batch [40/391] - Loss: 0.0008 - Train Accuracy: 0.9980\n",
      "Batch [50/391] - Loss: 0.0064 - Train Accuracy: 0.9981\n",
      "Batch [60/391] - Loss: 0.0004 - Train Accuracy: 0.9979\n",
      "Batch [70/391] - Loss: 0.0009 - Train Accuracy: 0.9975\n",
      "Batch [80/391] - Loss: 0.0067 - Train Accuracy: 0.9977\n",
      "Batch [90/391] - Loss: 0.0041 - Train Accuracy: 0.9976\n",
      "Batch [100/391] - Loss: 0.0008 - Train Accuracy: 0.9978\n",
      "Batch [110/391] - Loss: 0.0010 - Train Accuracy: 0.9979\n",
      "Batch [120/391] - Loss: 0.0031 - Train Accuracy: 0.9980\n",
      "Batch [130/391] - Loss: 0.0005 - Train Accuracy: 0.9982\n",
      "Batch [140/391] - Loss: 0.0001 - Train Accuracy: 0.9983\n",
      "Batch [150/391] - Loss: 0.0008 - Train Accuracy: 0.9984\n",
      "Batch [160/391] - Loss: 0.0114 - Train Accuracy: 0.9984\n",
      "Batch [170/391] - Loss: 0.0020 - Train Accuracy: 0.9983\n",
      "Batch [180/391] - Loss: 0.0002 - Train Accuracy: 0.9981\n",
      "Batch [190/391] - Loss: 0.0004 - Train Accuracy: 0.9981\n",
      "Batch [200/391] - Loss: 0.0018 - Train Accuracy: 0.9980\n",
      "Batch [210/391] - Loss: 0.0002 - Train Accuracy: 0.9981\n",
      "Batch [220/391] - Loss: 0.0004 - Train Accuracy: 0.9982\n",
      "Batch [230/391] - Loss: 0.0006 - Train Accuracy: 0.9983\n",
      "Batch [240/391] - Loss: 0.0036 - Train Accuracy: 0.9982\n",
      "Batch [250/391] - Loss: 0.0076 - Train Accuracy: 0.9982\n",
      "Batch [260/391] - Loss: 0.0213 - Train Accuracy: 0.9982\n",
      "Batch [270/391] - Loss: 0.0005 - Train Accuracy: 0.9981\n",
      "Batch [280/391] - Loss: 0.0016 - Train Accuracy: 0.9982\n",
      "Batch [290/391] - Loss: 0.0001 - Train Accuracy: 0.9983\n",
      "Batch [300/391] - Loss: 0.0119 - Train Accuracy: 0.9982\n",
      "Batch [310/391] - Loss: 0.0001 - Train Accuracy: 0.9982\n",
      "Batch [320/391] - Loss: 0.0001 - Train Accuracy: 0.9983\n",
      "Batch [330/391] - Loss: 0.0154 - Train Accuracy: 0.9982\n",
      "Batch [340/391] - Loss: 0.0083 - Train Accuracy: 0.9982\n",
      "Batch [350/391] - Loss: 0.0001 - Train Accuracy: 0.9982\n",
      "Batch [360/391] - Loss: 0.0026 - Train Accuracy: 0.9983\n",
      "Batch [370/391] - Loss: 0.0229 - Train Accuracy: 0.9982\n",
      "Batch [380/391] - Loss: 0.0001 - Train Accuracy: 0.9983\n",
      "Batch [390/391] - Loss: 0.0212 - Train Accuracy: 0.9982\n",
      "\n",
      "Validation Loss: 0.3775 - Validation Accuracy: 0.9268\n",
      "Epoch [17/20] - Train Loss: 0.0046 - Train Accuracy: 0.9982 - Val Loss: 0.3775 - Val Accuracy: 0.9268 - LR: 0.000040\n",
      "\n",
      "Epoch [18/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0142 - Train Accuracy: 0.9953\n",
      "Batch [20/391] - Loss: 0.0006 - Train Accuracy: 0.9977\n",
      "Batch [30/391] - Loss: 0.0001 - Train Accuracy: 0.9984\n",
      "Batch [40/391] - Loss: 0.0005 - Train Accuracy: 0.9984\n",
      "Batch [50/391] - Loss: 0.0037 - Train Accuracy: 0.9975\n",
      "Batch [60/391] - Loss: 0.0037 - Train Accuracy: 0.9977\n",
      "Batch [70/391] - Loss: 0.0001 - Train Accuracy: 0.9975\n",
      "Batch [80/391] - Loss: 0.0023 - Train Accuracy: 0.9975\n",
      "Batch [90/391] - Loss: 0.0001 - Train Accuracy: 0.9977\n",
      "Batch [100/391] - Loss: 0.0041 - Train Accuracy: 0.9978\n",
      "Batch [110/391] - Loss: 0.0001 - Train Accuracy: 0.9980\n",
      "Batch [120/391] - Loss: 0.0056 - Train Accuracy: 0.9982\n",
      "Batch [130/391] - Loss: 0.0070 - Train Accuracy: 0.9982\n",
      "Batch [140/391] - Loss: 0.0057 - Train Accuracy: 0.9983\n",
      "Batch [150/391] - Loss: 0.0180 - Train Accuracy: 0.9982\n",
      "Batch [160/391] - Loss: 0.0033 - Train Accuracy: 0.9983\n",
      "Batch [170/391] - Loss: 0.0004 - Train Accuracy: 0.9984\n",
      "Batch [180/391] - Loss: 0.0001 - Train Accuracy: 0.9985\n",
      "Batch [190/391] - Loss: 0.0103 - Train Accuracy: 0.9986\n",
      "Batch [200/391] - Loss: 0.0011 - Train Accuracy: 0.9986\n",
      "Batch [210/391] - Loss: 0.0002 - Train Accuracy: 0.9987\n",
      "Batch [220/391] - Loss: 0.0089 - Train Accuracy: 0.9987\n",
      "Batch [230/391] - Loss: 0.0003 - Train Accuracy: 0.9988\n",
      "Batch [240/391] - Loss: 0.0003 - Train Accuracy: 0.9988\n",
      "Batch [250/391] - Loss: 0.0002 - Train Accuracy: 0.9988\n",
      "Batch [260/391] - Loss: 0.0043 - Train Accuracy: 0.9988\n",
      "Batch [270/391] - Loss: 0.0001 - Train Accuracy: 0.9987\n",
      "Batch [280/391] - Loss: 0.0014 - Train Accuracy: 0.9986\n",
      "Batch [290/391] - Loss: 0.0007 - Train Accuracy: 0.9987\n",
      "Batch [300/391] - Loss: 0.0031 - Train Accuracy: 0.9986\n",
      "Batch [310/391] - Loss: 0.0001 - Train Accuracy: 0.9986\n",
      "Batch [320/391] - Loss: 0.0001 - Train Accuracy: 0.9987\n",
      "Batch [330/391] - Loss: 0.0001 - Train Accuracy: 0.9986\n",
      "Batch [340/391] - Loss: 0.0103 - Train Accuracy: 0.9986\n",
      "Batch [350/391] - Loss: 0.0003 - Train Accuracy: 0.9986\n",
      "Batch [360/391] - Loss: 0.0005 - Train Accuracy: 0.9986\n",
      "Batch [370/391] - Loss: 0.0001 - Train Accuracy: 0.9986\n",
      "Batch [380/391] - Loss: 0.0001 - Train Accuracy: 0.9986\n",
      "Batch [390/391] - Loss: 0.0000 - Train Accuracy: 0.9986\n",
      "\n",
      "Validation Loss: 0.4008 - Validation Accuracy: 0.9268\n",
      "Epoch [18/20] - Train Loss: 0.0038 - Train Accuracy: 0.9986 - Val Loss: 0.4008 - Val Accuracy: 0.9268 - LR: 0.000040\n",
      "\n",
      "Epoch [19/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0008 - Train Accuracy: 0.9984\n",
      "Batch [20/391] - Loss: 0.0032 - Train Accuracy: 0.9992\n",
      "Batch [30/391] - Loss: 0.0001 - Train Accuracy: 0.9984\n",
      "Batch [40/391] - Loss: 0.0011 - Train Accuracy: 0.9988\n",
      "Batch [50/391] - Loss: 0.0094 - Train Accuracy: 0.9991\n",
      "Batch [60/391] - Loss: 0.0000 - Train Accuracy: 0.9992\n",
      "Batch [70/391] - Loss: 0.0002 - Train Accuracy: 0.9987\n",
      "Batch [80/391] - Loss: 0.0058 - Train Accuracy: 0.9986\n",
      "Batch [90/391] - Loss: 0.0075 - Train Accuracy: 0.9986\n",
      "Batch [100/391] - Loss: 0.0029 - Train Accuracy: 0.9984\n",
      "Batch [110/391] - Loss: 0.0002 - Train Accuracy: 0.9983\n",
      "Batch [120/391] - Loss: 0.0003 - Train Accuracy: 0.9983\n",
      "Batch [130/391] - Loss: 0.0029 - Train Accuracy: 0.9984\n",
      "Batch [140/391] - Loss: 0.0039 - Train Accuracy: 0.9985\n",
      "Batch [150/391] - Loss: 0.0000 - Train Accuracy: 0.9985\n",
      "Batch [160/391] - Loss: 0.0006 - Train Accuracy: 0.9986\n",
      "Batch [170/391] - Loss: 0.0020 - Train Accuracy: 0.9986\n",
      "Batch [180/391] - Loss: 0.0000 - Train Accuracy: 0.9986\n",
      "Batch [190/391] - Loss: 0.0003 - Train Accuracy: 0.9986\n",
      "Batch [200/391] - Loss: 0.0001 - Train Accuracy: 0.9985\n",
      "Batch [210/391] - Loss: 0.0051 - Train Accuracy: 0.9985\n",
      "Batch [220/391] - Loss: 0.0013 - Train Accuracy: 0.9986\n",
      "Batch [230/391] - Loss: 0.0142 - Train Accuracy: 0.9986\n",
      "Batch [240/391] - Loss: 0.0012 - Train Accuracy: 0.9986\n",
      "Batch [250/391] - Loss: 0.0004 - Train Accuracy: 0.9986\n",
      "Batch [260/391] - Loss: 0.0003 - Train Accuracy: 0.9986\n",
      "Batch [270/391] - Loss: 0.0003 - Train Accuracy: 0.9987\n",
      "Batch [280/391] - Loss: 0.0080 - Train Accuracy: 0.9987\n",
      "Batch [290/391] - Loss: 0.0052 - Train Accuracy: 0.9987\n",
      "Batch [300/391] - Loss: 0.0005 - Train Accuracy: 0.9985\n",
      "Batch [310/391] - Loss: 0.0000 - Train Accuracy: 0.9984\n",
      "Batch [320/391] - Loss: 0.0000 - Train Accuracy: 0.9984\n",
      "Batch [330/391] - Loss: 0.0225 - Train Accuracy: 0.9984\n",
      "Batch [340/391] - Loss: 0.0000 - Train Accuracy: 0.9985\n",
      "Batch [350/391] - Loss: 0.0001 - Train Accuracy: 0.9985\n",
      "Batch [360/391] - Loss: 0.0113 - Train Accuracy: 0.9985\n",
      "Batch [370/391] - Loss: 0.0053 - Train Accuracy: 0.9985\n",
      "Batch [380/391] - Loss: 0.0024 - Train Accuracy: 0.9985\n",
      "Batch [390/391] - Loss: 0.0026 - Train Accuracy: 0.9985\n",
      "\n",
      "Validation Loss: 0.4169 - Validation Accuracy: 0.9268\n",
      "Epoch [19/20] - Train Loss: 0.0043 - Train Accuracy: 0.9985 - Val Loss: 0.4169 - Val Accuracy: 0.9268 - LR: 0.000040\n",
      "\n",
      "Epoch [20/20] - Training...\n",
      "Batch [10/391] - Loss: 0.0294 - Train Accuracy: 0.9984\n",
      "Batch [20/391] - Loss: 0.0023 - Train Accuracy: 0.9984\n",
      "Batch [30/391] - Loss: 0.0050 - Train Accuracy: 0.9990\n",
      "Batch [40/391] - Loss: 0.0003 - Train Accuracy: 0.9992\n",
      "Batch [50/391] - Loss: 0.0006 - Train Accuracy: 0.9988\n",
      "Batch [60/391] - Loss: 0.0433 - Train Accuracy: 0.9987\n",
      "Batch [70/391] - Loss: 0.0052 - Train Accuracy: 0.9982\n",
      "Batch [80/391] - Loss: 0.0012 - Train Accuracy: 0.9984\n",
      "Batch [90/391] - Loss: 0.0708 - Train Accuracy: 0.9981\n",
      "Batch [100/391] - Loss: 0.0001 - Train Accuracy: 0.9980\n",
      "Batch [110/391] - Loss: 0.0002 - Train Accuracy: 0.9982\n",
      "Batch [120/391] - Loss: 0.0086 - Train Accuracy: 0.9982\n",
      "Batch [130/391] - Loss: 0.0000 - Train Accuracy: 0.9981\n",
      "Batch [140/391] - Loss: 0.0008 - Train Accuracy: 0.9977\n",
      "Batch [150/391] - Loss: 0.0093 - Train Accuracy: 0.9975\n",
      "Batch [160/391] - Loss: 0.0056 - Train Accuracy: 0.9977\n",
      "Batch [170/391] - Loss: 0.0000 - Train Accuracy: 0.9978\n",
      "Batch [180/391] - Loss: 0.0000 - Train Accuracy: 0.9979\n",
      "Batch [190/391] - Loss: 0.0012 - Train Accuracy: 0.9980\n",
      "Batch [200/391] - Loss: 0.0009 - Train Accuracy: 0.9981\n",
      "Batch [210/391] - Loss: 0.0008 - Train Accuracy: 0.9981\n",
      "Batch [220/391] - Loss: 0.0004 - Train Accuracy: 0.9981\n",
      "Batch [230/391] - Loss: 0.0000 - Train Accuracy: 0.9981\n",
      "Batch [240/391] - Loss: 0.0001 - Train Accuracy: 0.9982\n",
      "Batch [250/391] - Loss: 0.0000 - Train Accuracy: 0.9982\n",
      "Batch [260/391] - Loss: 0.0000 - Train Accuracy: 0.9983\n",
      "Batch [270/391] - Loss: 0.0012 - Train Accuracy: 0.9983\n",
      "Batch [280/391] - Loss: 0.0043 - Train Accuracy: 0.9982\n",
      "Batch [290/391] - Loss: 0.0000 - Train Accuracy: 0.9982\n",
      "Batch [300/391] - Loss: 0.0248 - Train Accuracy: 0.9982\n",
      "Batch [310/391] - Loss: 0.0004 - Train Accuracy: 0.9982\n",
      "Batch [320/391] - Loss: 0.0017 - Train Accuracy: 0.9982\n",
      "Batch [330/391] - Loss: 0.0015 - Train Accuracy: 0.9983\n",
      "Batch [340/391] - Loss: 0.0000 - Train Accuracy: 0.9983\n",
      "Batch [350/391] - Loss: 0.0042 - Train Accuracy: 0.9983\n",
      "Batch [360/391] - Loss: 0.0134 - Train Accuracy: 0.9983\n",
      "Batch [370/391] - Loss: 0.0014 - Train Accuracy: 0.9983\n",
      "Batch [380/391] - Loss: 0.0000 - Train Accuracy: 0.9984\n",
      "Batch [390/391] - Loss: 0.0001 - Train Accuracy: 0.9984\n",
      "\n",
      "Validation Loss: 0.4156 - Validation Accuracy: 0.9276\n",
      "Epoch [20/20] - Train Loss: 0.0043 - Train Accuracy: 0.9984 - Val Loss: 0.4156 - Val Accuracy: 0.9276 - LR: 0.000008\n",
      "\n",
      "Final Test Evaluation:\n",
      "\n",
      "Test Loss: 0.4336 - Test Accuracy: 0.9284\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93      2500\n",
      "           1       0.92      0.94      0.93      2500\n",
      "\n",
      "    accuracy                           0.93      5000\n",
      "   macro avg       0.93      0.93      0.93      5000\n",
      "weighted avg       0.93      0.93      0.93      5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.43363695738613606, 0.9284)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = \"path to train dataset\"\n",
    "val_dir = \"path to valid dataset\"\n",
    "test_dir = \"path to test dataset\"\n",
    "\n",
    "    # Transforms for better learning\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "    # Create datasets\n",
    "train_dataset = CustomDataset(ImageFolder(train_dir), train_transform)\n",
    "val_dataset = CustomDataset(ImageFolder(val_dir), val_test_transform)\n",
    "test_dataset = CustomDataset(ImageFolder(test_dir), val_test_transform)\n",
    "\n",
    "    # Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    # Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepfakeDetectionModel().to(device)\n",
    "\n",
    "    # Train the model\n",
    "model.train_model(train_loader, val_loader, num_epochs=20, device=device)\n",
    "\n",
    "    # Final evaluation\n",
    "print(\"\\nFinal Test Evaluation:\")\n",
    "model.evaluate_model(test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
